{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee61837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "130ac237",
   "metadata": {},
   "source": [
    "# Capstone  - Fake Job Posting - Data Wrangling and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c193c76b",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "1. [Background](#Background)\n",
    "     -   1.1 [Data Source](#Data-Source)\n",
    "     -   1.2 [Objective](#Objective)\n",
    "     \n",
    "     \n",
    "2. [Loading Data](#Loading-Data)\n",
    "     -   2.1 [Load libraries](#Load-libraries)\n",
    "     -   2.2 [Load Dataset](#Load-Dataset)\n",
    "     \n",
    "     \n",
    "3. [Data Quality Check](#Data-Quality-Check)\n",
    "     -   3.1 [Duplicate Values](#Duplicate-Values)\n",
    "     -   3.2 [Missing Values](#Missing-Values)\n",
    "     \n",
    "     \n",
    "4. [Data Wrangling](#Data-Wrangling)\n",
    "     -   4.1 [Split the location in Country, State and City](#Split-the-location-in-Country-State-and-City)\n",
    "     -   4.2 [Split the salary range column to minimum and maximum](#Split-the-salary-range-column-to-minimum-and-maximum)\n",
    "     -   4.3 [Dealing with missing values](#Dealing-with-missing-values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48ff624",
   "metadata": {},
   "source": [
    "## Background\n",
    "    \n",
    "Scammers advertise jobs the same way legitimate employers do—online (in ads, on job sites, college employment sites, and social media), in newspapers, and sometimes on TV and radio. They promise you a job, but what they want is your money and your personal information.\n",
    "\n",
    "Fake Job or Employment Scams occur when criminal actors deceive victims into believing they have a job or a potential job. Criminals leverage their position as “employers” to persuade victims to provide them with personally identifiable information (PII), become unwitting money mules, or to send them money.\n",
    "\n",
    "Fake Job Scams have existed for a long time but technology has made this scam easier and more lucrative. Cyber criminals now pose as legitimate employers by spoofing company websites and posting fake job openings on popular online job boards. They conduct false interviews with unsuspecting applicant victims, then request PII and/or money from these individuals. \n",
    "\n",
    "\n",
    "https://www.fbi.gov/contact-us/field-offices/elpaso/news/press-releases/fbi-warns-cyber-criminals-are-using-fake-job-listings-to-target-applicants-personally-identifiable-information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3c13d5",
   "metadata": {},
   "source": [
    "## What is a Fake Job Posting?\n",
    "\n",
    "A fake job posting is a (rarely) smartly designed type of scam aimed at job seekers for a variety of unprofessional reasons. Still, these scams can look legit to an unsuspicious person scrolling through the vast pool of jobs. And although most tech talents aren’t actively looking for a new employer, falling for a phantom ad is still realistic. How so?\n",
    "\n",
    "Scammers will sometimes go the extra mile to draw the attention of their target audience, more often than not, by offering incredibly high salary ranges or another sort of advantage that seems too good to be true. So, make sure to remember this: when a JD seems like a dream come true, do a thorough background check on the company or recruitment agency advertising it. Search through their website, social media, and various job boards before you take a leap of faith and end up wasting your time on a dead-end hiring process, or worse. \n",
    "\n",
    "https://www.omnesgroup.com/fake-job-posting/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cb8002",
   "metadata": {},
   "source": [
    "#### 1.1 Data Source\n",
    "\n",
    "This dataset contains 18K job descriptions out of which about 800 are fake. The data consists of both textual information and meta-information about the jobs. The dataset can be used to create classification models which can learn the job descriptions which are fraudulent.\n",
    "\n",
    "https://www.kaggle.com/datasets/shivamb/real-or-fake-fake-jobposting-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0666a29",
   "metadata": {},
   "source": [
    "#### 1.2 Problem Statement / Objective\n",
    "To predicit fradulent job posting in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86cd97f",
   "metadata": {},
   "source": [
    "## Loading Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa772d9d",
   "metadata": {},
   "source": [
    "#### 2.1 Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_theme(style='darkgrid')\n",
    "import matplotlib.pyplot as plt\n",
    "import collections, re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dcf7f5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <strong>Success!</strong> Successfully loaded all the required libraries.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963e9597",
   "metadata": {},
   "source": [
    "#### 2.2 Load Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11246eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fake_job_postings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736ab7a4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <strong>Success!</strong> Successfully loaded data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c16296",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d28c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc79d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce4d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea5c34c",
   "metadata": {},
   "source": [
    "## Data Quality Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f4b97c",
   "metadata": {},
   "source": [
    "#### 3.1 Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78bf440",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df[df.duplicated(keep=False)]) == 0:\n",
    "    print(\"There is no duplicated records in the fake job posting dataset\")\n",
    "else:\n",
    "    print(\"There are duplicated records in the fake job posting dataset. Please indetify the reasons and work to fix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac06407e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <strong>Success!</strong> There is no duplicated values in the dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9483dad",
   "metadata": {},
   "source": [
    "#### 3.2 Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c1eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \\nCount total NaN at each column in a DataFrame : \\n\\n\",\n",
    "df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a4b4d",
   "metadata": {},
   "source": [
    "#### 3.3  Graphical representation of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3526e60f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "null_values = df.isnull().sum()\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.barplot(null_values.index, null_values, color = 'grey')\n",
    "plt.ylabel('Missing values count', fontsize = 15)\n",
    "plt.xticks(rotation = '90', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa4bad0",
   "metadata": {},
   "source": [
    "## Data Wrangling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09384f67",
   "metadata": {},
   "source": [
    "#### Job ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e673fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique job_id \n",
    "len(pd.unique(df['job_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865958ea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>Info!</strong> job_id is unique identifer in the dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72792e9c",
   "metadata": {},
   "source": [
    "#### Location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5abc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = df[\"location\"].str.split(\",\", expand= True, n= 2)\n",
    "location.columns = [\"country\", \"state\", \"city\"]\n",
    "df[[\"country\", \"state\", \"city\"]] = location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dbd56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the original location column from the dataset\n",
    "df = df.drop(columns= \"location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b746b22",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <strong>Success!</strong> dropped the original location column and splitted location into City, State and Country\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65d348c",
   "metadata": {},
   "source": [
    "#### Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3035bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique department\n",
    "print(df['department'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3954fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['department'] =df['department'].fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889306a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['department']=df['department'].str.lower()\n",
    "regex = re.compile('[@_!#$%^&*()<>?/\\|}{~:]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed7e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(subject):\n",
    "    clean_tokens = re.findall(r\"(?i)\\b[a-z]+\\b\", subject)\n",
    "    clean_s = ' '.join(clean_tokens)\n",
    "    return clean_s\n",
    "\n",
    "# source: https://github.com/TommyJiang91/Fake_Job_Posting_Detection/blob/master/Data_Cleaning_and_Salary_Matching_Final.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea96c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['department']= df['department'].apply(lambda x: clean_string(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711bb574",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['department'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dda71a3",
   "metadata": {},
   "source": [
    "#### Salary Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc203db",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary = df[\"salary_range\"].str.split(\"-\", expand= True, n= 1)\n",
    "df[[\"min_salary\", \"max_salary\"]] = salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac03d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns= \"salary_range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1e79a6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <strong>Success!</strong> Splitted the salary_range into minimum and maxium column and dropped the original salary_range column.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f186f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256d09d8",
   "metadata": {},
   "source": [
    "#### Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88846d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chnage the datatype\n",
    "df.description = df.description.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference: https://www.andyfitzgeraldconsulting.com/writing/keyword-extraction-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c5ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of stop words from nltk\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(sorted(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823cfd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process dataset to get a cleaned and normalised text corpus\n",
    "# Add word count for description column in the dataset\n",
    "corpus_desc = []\n",
    "df['desc_word_count'] = df['description'].apply(lambda x: len(str(x).split(\" \")))\n",
    "ds_count = len(df.desc_word_count)\n",
    "for i in range(0, ds_count):\n",
    "    # Remove punctuation\n",
    "    text = re.sub('[^a-zA-Z]', ' ', str(df['description'][i]))\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    # Convert to list from string\n",
    "    text = text.split()\n",
    "    \n",
    "    # Stemming\n",
    "    ps=PorterStemmer()\n",
    "    \n",
    "    # Lemmatisation\n",
    "    lem = WordNetLemmatizer()\n",
    "    text = [lem.lemmatize(word) for word in text if not word in  \n",
    "            stop_words] \n",
    "    text = \" \".join(text)\n",
    "    corpus_desc.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5662cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word cloud for description \n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "wordcloud_desc = WordCloud(\n",
    "                          background_color='white',\n",
    "                          stopwords=stop_words,\n",
    "                          max_words=200,\n",
    "                          max_font_size=50, \n",
    "                          random_state=42\n",
    "                         ).generate(str(corpus_desc))\n",
    "print(wordcloud_desc)\n",
    "plt.figure( figsize=(20,10) )\n",
    "plt.imshow(wordcloud_desc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb85fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text and build a vocabulary of known words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
    "X=cv.fit_transform(corpus_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the returned vector encoding the length of the entire vocabulary\n",
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a6c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View most frequently occuring keywords\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus_desc)\n",
    "    bag_of_words = vec.transform(corpus_desc)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
    "                   vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                       reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Convert most freq words to dataframe for plotting bar plot, save as CSV\n",
    "top_words = get_top_n_words(corpus_desc, n=20)\n",
    "top_df = pd.DataFrame(top_words)\n",
    "top_df.columns=[\"Keyword\", \"Frequency\"]\n",
    "\n",
    "\n",
    "# Barplot of most freq words\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "g = sns.barplot(x=\"Keyword\", y=\"Frequency\", data=top_df, palette=\"Blues_d\")\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7642b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequently occuring bigrams\n",
    "def get_top_n2_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(2,2),  \n",
    "            max_features=2000).fit(corpus_desc)\n",
    "    bag_of_words = vec1.transform(corpus_desc)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Convert most freq bigrams to dataframe for plotting bar plot, save as CSV\n",
    "top2_words = get_top_n2_words(corpus_desc, n=20)\n",
    "top2_df = pd.DataFrame(top2_words)\n",
    "top2_df.columns=[\"Bi-gram\", \"Frequency\"]\n",
    "\n",
    "\n",
    "# Barplot of most freq Bi-grams\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "h=sns.barplot(x=\"Bi-gram\", y=\"Frequency\", data=top2_df, palette=\"Blues_d\")\n",
    "h.set_xticklabels(h.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bdc01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequently occuring Tri-grams\n",
    "def get_top_n3_words(corpus_desc, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(3,3), \n",
    "           max_features=2000).fit(corpus_desc)\n",
    "    bag_of_words = vec1.transform(corpus_desc)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Convert most freq trigrams to dataframe for plotting bar plot, save as CSV\n",
    "top3_words = get_top_n3_words(corpus_desc, n=20)\n",
    "top3_df = pd.DataFrame(top3_words)\n",
    "top3_df.columns=[\"Tri-gram\", \"Frequency\"]\n",
    "\n",
    "# Barplot of most freq Tri-grams\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "j=sns.barplot(x=\"Tri-gram\", y=\"Frequency\", data=top3_df, palette=\"Blues_d\")\n",
    "j.set_xticklabels(j.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74debca7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>Info!</strong> Did not find meaningful keywords. Probabaloy, its would be a good idea to find keywords based on industry. Due to limited time, I will leave it for future.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88e2961",
   "metadata": {},
   "source": [
    "#### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cacba10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c334791c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db701906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372fefc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95f83e23",
   "metadata": {},
   "source": [
    "#### Benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2245658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique department\n",
    "print(df['benefits'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09303c",
   "metadata": {},
   "source": [
    "###### Extracting Keywords fron Benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c300b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process dataset to get a cleaned and normalised text corpus\n",
    "corpus = []\n",
    "df['word_count_benefits'] = df['benefits'].apply(lambda x: len(str(x).split(\" \")))\n",
    "ds_count = len(df.word_count_benefits)\n",
    "for i in range(0, ds_count):\n",
    "    # Remove punctuation\n",
    "    text = re.sub('[^a-zA-Z]', ' ', str(df['benefits'][i]))\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    # Convert to list from string\n",
    "    text = text.split()\n",
    "    \n",
    "    # Stemming\n",
    "    ps=PorterStemmer()\n",
    "    \n",
    "    # Lemmatisation\n",
    "    lem = WordNetLemmatizer()\n",
    "    text = [lem.lemmatize(word) for word in text if not word in  \n",
    "            stop_words] \n",
    "    text = \" \".join(text)\n",
    "    corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020572ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View sample pre-processed corpus item\n",
    "corpus[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c62ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word cloud\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "wordcloud = WordCloud(\n",
    "                          background_color='white',\n",
    "                          stopwords=stop_words,\n",
    "                          max_words=100,\n",
    "                          max_font_size=50, \n",
    "                          random_state=42\n",
    "                         ).generate(str(corpus))\n",
    "print(wordcloud)\n",
    "plt.figure( figsize=(20,10) )\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeba212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text and build a vocabulary of known words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
    "X=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897fbc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the returned vector encoding the length of the entire vocabulary\n",
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25bfc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View most frequently occuring keywords\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
    "                   vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                       reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Convert most freq words to dataframe for plotting bar plot, save as CSV\n",
    "top_words = get_top_n_words(corpus, n=20)\n",
    "top_df = pd.DataFrame(top_words)\n",
    "top_df.columns=[\"Keyword\", \"Frequency\"]\n",
    "\n",
    "# Barplot of most freq words\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "g = sns.barplot(x=\"Keyword\", y=\"Frequency\", data=top_df, palette=\"Blues_d\")\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d940ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequently occuring bigrams\n",
    "def get_top_n2_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(2,2),  \n",
    "            max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Convert most freq bigrams to dataframe for plotting bar plot, save as CSV\n",
    "top2_words = get_top_n2_words(corpus, n=20)\n",
    "top2_df = pd.DataFrame(top2_words)\n",
    "top2_df.columns=[\"Bi-gram\", \"Frequency\"]\n",
    "\n",
    "# Barplot of most freq Bi-grams\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "h=sns.barplot(x=\"Bi-gram\", y=\"Frequency\", data=top2_df, palette=\"Blues_d\")\n",
    "h.set_xticklabels(h.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c9f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequently occuring Tri-grams\n",
    "def get_top_n3_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(3,3), \n",
    "           max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Convert most freq trigrams to dataframe for plotting bar plot, save as CSV\n",
    "top3_words = get_top_n3_words(corpus, n=20)\n",
    "top3_df = pd.DataFrame(top3_words)\n",
    "top3_df.columns=[\"Tri-gram\", \"Frequency\"]\n",
    "\n",
    "# Barplot of most freq Tri-grams\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "j=sns.barplot(x=\"Tri-gram\", y=\"Frequency\", data=top3_df, palette=\"Blues_d\")\n",
    "j.set_xticklabels(j.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bdbdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TF-IDF (term frequency/inverse document frequency) -- \n",
    "# TF-IDF lists word frequency scores that highlight words that \n",
    "# are more important to the context rather than those that \n",
    "# appear frequently across documents\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(X)\n",
    "\n",
    "# Get feature names\n",
    "feature_names=cv.get_feature_names()\n",
    " \n",
    "# Fetch document for which keywords needs to be extracted\n",
    "doc=corpus[ds_count-1]\n",
    " \n",
    "# Generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f58b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort tf_idf in descending order\n",
    "from scipy.sparse import coo_matrix\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=25):\n",
    "    \n",
    "    # Use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # Word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        # Keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    # Create tuples of feature,score\n",
    "    # Results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    return results\n",
    "\n",
    "# Sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "# Extract only the top n; n here is 25\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,25)\n",
    " \n",
    "# Print the results, save as CSV\n",
    "print(\"\\nAbstract:\")\n",
    "print(doc)\n",
    "print(\"\\nKeywords:\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])\n",
    "\n",
    "# import csv\n",
    "# with open(file_prefix + 'td_idf.csv', 'w', newline=\"\") as csv_file:  \n",
    "#     writer = csv.writer(csv_file)\n",
    "#     writer.writerow([\"Keyword\", \"Importance\"])\n",
    "#     for key, value in keywords.items():\n",
    "#        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8924f1b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>Info!</strong> Benefits Keywords are 'benefits', 'mental','dental','visison', 'life insurance', 'health insurance', 'work life balance', 'long term disability', 'stock options', 'work life balance'.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544863c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "benefits_kw = ['benefits', 'mental','dental','visison', 'life insurance', 'health insurance', 'work life balance', 'long term disability', 'stock options', 'work life balance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74804bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing values\n",
    "df['benefits'] = df['benefits'].fillna(\"None\")\n",
    "\n",
    "# Now we will convert it into 'int64' type.\n",
    "df.benefits = df.benefits.astype('str')\n",
    "\n",
    "#count the keywords and create a column with\n",
    "df['benefit_kw_count']=df['benefits'].str.findall('|'.join(benefits_kw)).str.len()\n",
    "\n",
    "df['benefit_kw_count'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fd596e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>Info!</strong> 14184 job postings donot have the benefits keywords remaining 3705 have one or more benefits keywords\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6301d9b1",
   "metadata": {},
   "source": [
    "#### Employement Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c178c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing employement type with 'none'\n",
    "df['employment_type'] =df['employment_type'].fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f07e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of employment type\n",
    "print(df['employment_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcb13f1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <strong>Success!</strong> filled the missing employment type with None.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f3e75f",
   "metadata": {},
   "source": [
    "#### Required Experience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6582794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['required_experience'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f8fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing required experience with 'none'\n",
    "df['required_experience'] =df['required_experience'].fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of required experience \n",
    "print(df['required_experience'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f67d6f",
   "metadata": {},
   "source": [
    "#### Required Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6971087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['required_education'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing required education with 'none'\n",
    "df['required_education'] =df['required_education'].fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeffa73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of required experience \n",
    "print(df['required_education'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01842fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming some of the required education for consistency\n",
    "df[\"required_education\"]=df[\"required_education\"].replace(\"Vocational - Degree\", \"Vocational\")\n",
    "df[\"required_education\"]=df[\"required_education\"].replace(\"Vocational - HS Diploma\", \"High School\")\n",
    "df[\"required_education\"]=df[\"required_education\"].replace(\"Some High School Coursework\", \"High School\")\n",
    "df[\"required_education\"]=df[\"required_education\"].replace(\"High School or equivalent\", \"High School\")\n",
    "df[\"required_education\"]=df[\"required_education\"].replace(\"Some College Coursework Completed\", \"Associate\")\n",
    "df[\"required_education\"]=df[\"required_education\"].replace(\"Unspecified\", \"None\")\n",
    "df[\"required_education\"]=df[\"required_education\"].replace(\"Bachelor's Degree\", \"Bachelor's\")\n",
    "df[\"required_education\"]=df[\"required_education\"].replace(\"Master's Degree\", \"Master's\")\n",
    "df[\"required_education\"]=df[\"required_education\"].replace(\"Associate Degree\", \"Associate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aecca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of required experience \n",
    "print(df['required_education'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918d34a5",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a2b98",
   "metadata": {},
   "source": [
    "#### 5.1 Count of real and fradulent job posting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ad41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df.fraudulent,palette=[\"#0b5394\", \"#9fc5e8\"]).set_title('Real & Fradulent')\n",
    "df.groupby('fraudulent').count()['title'].reset_index().sort_values(by='title',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480a755",
   "metadata": {},
   "source": [
    "#### 5.4 Count of real and fraudulent job posting by department "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01bae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='department', data=df, hue=\"fraudulent\", palette=[\"#0b5394\", \"#9fc5e8\"], order=df['department'].value_counts().iloc[:10].index)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b702308",
   "metadata": {},
   "source": [
    "#### 5.2 Count of real and fraudulent job posting by Country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602e3211",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='country', data=df, hue=\"fraudulent\", palette=[\"#0b5394\", \"#9fc5e8\"], order=df['country'].value_counts().iloc[:10].index)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a32d0",
   "metadata": {},
   "source": [
    "#### 5.3 Count of real and fraudulent job posting by Employment Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d341f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the \"employment_type\" column cross table regarding the target variable in a normalized form\n",
    "employment_type_cross = pd.crosstab(df[\"employment_type\"], df[\"fraudulent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f509db",
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_type_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c60dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='employment_type', data=df, hue=\"fraudulent\", palette=[\"#0b5394\", \"#9fc5e8\"], order=df['employment_type'].value_counts().iloc[:10].index)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb10ae5",
   "metadata": {},
   "source": [
    "#### 5.5 Count of real and fraudulent job posting by telecommuting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae38ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='telecommuting', data=df, hue=\"fraudulent\", palette=[\"#0b5394\", \"#9fc5e8\"], order=df['telecommuting'].value_counts().iloc[:10].index)\n",
    "plt.xticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f8f54",
   "metadata": {},
   "source": [
    "#### 5.6 Count of real and fraudulent job posting by has_company_logo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87535381",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='has_company_logo', data=df, hue=\"fraudulent\", palette=[\"#0b5394\", \"#9fc5e8\"], order=df['has_company_logo'].value_counts().iloc[:10].index)\n",
    "plt.xticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d83a8f",
   "metadata": {},
   "source": [
    "#### 5.7 Count of real and fraudulent job posting by experience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bca5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\":(12, 6)}) #width=6, height=5\n",
    "sns.countplot(x='required_experience', data=df, hue=\"fraudulent\", palette=[\"#0b5394\", \"#9fc5e8\"], order=df['required_experience'].value_counts().iloc[:10].index)\n",
    "plt.xticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea10689",
   "metadata": {},
   "source": [
    "#### Count of real and Fake job posting by education "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cccc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\":(12, 6)}) #width=6, height=5\n",
    "sns.countplot(x='required_education', data=df, hue=\"fraudulent\", palette=[\"#0b5394\", \"#9fc5e8\"], order=df['required_education'].value_counts().iloc[:10].index)\n",
    "plt.xticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8402a1a",
   "metadata": {},
   "source": [
    "#### 5.8 Count of real and fraudulent job posting by Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a35b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\":(15, 6)}) #width=6, height=5\n",
    "sns.countplot(x='function', data=df, hue=\"fraudulent\", palette=[\"#0b5394\", \"#9fc5e8\"], order=df['function'].value_counts().iloc[:10].index)\n",
    "plt.xticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57a4fd3",
   "metadata": {},
   "source": [
    "#### 5.9 Count of benefits keywords in Real and Fake job postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818fff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the comparision between benefit keywords in fake and real job postings\n",
    "sns.set(rc={\"figure.figsize\":(8,6)}) #width=6, height=5\n",
    "sns.countplot(x='benefit_kw_count', data=df, hue=\"fraudulent\", palette=[\"#0b5394\", \"#9fc5e8\"], order=df['benefit_kw_count'].value_counts().iloc[:4].index)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91532e55",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>Info!</strong> It looks like mostly both real and fake job postings have no benefits keywords. However, fake jobs have only unigrams \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2546a701",
   "metadata": {},
   "source": [
    "### Comparing number of characters in Real and Fake job postings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extracting Text Featurs\"\"\"\n",
    "\n",
    "text_df = df[[\"title\", \"company_profile\", \"description\", \"requirements\", \"benefits\",\"fraudulent\"]]\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba283b",
   "metadata": {},
   "source": [
    "#### 6.1 Characters in Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668c665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\n",
    "length=text_df[text_df[\"fraudulent\"]==1]['description'].str.len()\n",
    "ax1.hist(length,bins = 20,color='#0b5394')\n",
    "ax1.set_title('Fake Post')\n",
    "length=text_df[text_df[\"fraudulent\"]==0]['description'].str.len()\n",
    "ax2.hist(length, bins = 20,color = '#9fc5e8')\n",
    "ax2.set_title('Real Post')\n",
    "fig.suptitle('Characters in description')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386e7a71",
   "metadata": {},
   "source": [
    "The distribution of charaters in description of the fake and real post are similar but some fake post reach to 6000 to 6500 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6d8ad1",
   "metadata": {},
   "source": [
    "#### 6.2 Characters in Requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea3f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\n",
    "length=text_df[text_df[\"fraudulent\"]==1]['requirements'].str.len()\n",
    "ax1.hist(length,bins = 20,color='#0b5394')\n",
    "ax1.set_title('Fake Post')\n",
    "length=text_df[text_df[\"fraudulent\"]==0]['requirements'].str.len()\n",
    "ax2.hist(length,bins = 20, color = '#9fc5e8')\n",
    "ax2.set_title('Real Post')\n",
    "fig.suptitle('Characters in requirements')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
